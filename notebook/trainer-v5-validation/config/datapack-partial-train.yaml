# lightning.pytorch==2.0.2
seed_everything: 3941088705
trainer:

  #
  # Configure the deepspeed strategy, we recommend you start with `deepspeed_stage_2_offload` 
  # and adjust from there according to your training needs. `deepspeed_stage_3_offload` is useful  
  # for training LoRA on large models on a single GPU.
  #
  # In general you would want to use the following:
  #
  # - deepspeed_stage_1 : Each of your GPU has too much vram, and you do not know what to do
  #
  # - deepspeed_stage_2 : Optimal distributed training strategy, across multiple gpu each with sufficient vram
  # - deepspeed_stage_2_offload : Reduce vram usage by offloading the optimizer state and work to cpu
  #
  # - deepspeed_stage_3 : Split up the model across multiple gpu, useful for large models, at a performance cost
  # - deepspeed_stage_3_offload : Additional offloading, for even greater performance cost
  #
  # For more details see:
  # https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed-zero-stage-2
  #
  strategy: deepspeed_stage_2_offload

  # Logger setting for wandb, if you want to enable wandb, uncomment the whole logger section
  # ---
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      name: 'infctx-v5-unit-test-baseline (train-ctx=4096, data-ctx=4096)'
      project: 'RWKV-infctx-unit-test'
      tags: ['RWKV', 'infctx']
  
  # Checkpoint settings for the training process
  callbacks:
    class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      # Configure this to the path you want to save your checkpoints to
      # note that a subdir will be created with the name `epoch=x-step=y.ckpt`
      # 
      # to convert a checkpoint to a model, you can use the 
      # `python3 export_checkpoint.py <checkpoint path>` script, 
      # which will create a `rwkv_model.pth` in the checkpoint directory.
      #
      # Do not use the `zero_to_fp32.py` script as that will have export format issues
      dirpath: ../checkpoint/trainer-validaiton/datapack-validation/
      filename: null
      
      # Save the top/last K checkpoints
      save_top_k: 3
      # Choose by the most recent checkpoints (step based)
      monitor: 'step'
      mode: max
      
      # If enabled (true), save a copy of the latest checkpoint to 'last.ckpt'
      # useful to simply checkpoint resume scripts, at a price of disk performance
      save_last: false

      # DO NOT set this as true, as the model weight exported will have format issues
      # expert as checkpoint, and use the `export_checkpoint.py` script to convert to model instead
      save_weights_only: false

      # How frequent you want to save a checkpoint for every step.
      # This will happen for every X data sample, where X = every_n_train_steps * accumulate_grad_batches
      #
      # In general you will want to avoid putting a low number (expecially if accumulate_grad_batches <= 100)
      # as the checkpoint process, will pause all the gpu training for some time, slowing down the overall process
      # However you do not want to configure too high of a number, where you will lose too much progress if the training crashes
      every_n_train_steps: 100
      every_n_epochs: null
      save_on_train_epoch_end: true
      train_time_interval: null

      # Other settings, you can probably leave alone
      verbose: false
      auto_insert_metric_name: true
  
  ########################################
  ## Training run parameter settings
  ########################################

  # Generally what you want to configure is the maximum number of epochs
  # Leave it as -1, and it will keep going forever till interrupted
  # Or set it as a number, and it will stop after that number of epochs
  max_epochs: 1
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null

  # Number of datasamples to train for each step, a data sample is considered
  # a "substep" in wandb logs, and a "step" is tracked as "trainer/global_step"
  #
  # This decides the number of datasample, to learn together from, before backproping
  # any weight changes at the end of the batch.
  #
  # Recommended to be a big enough number (like 128/256) where it prevents the training 
  # loss from flucuating in the process. But not too big of a number where the increased
  # GPU vRAM / offloaded RAM usage will cause the training to crash.
  #
  # You are also recommended to configure this to a large enough number to fully utilize
  # your GPU processing time %, and avoid idle time for the GPU between batches
  #
  # This number is divided by the number of GPUs, and nodes configured
  # So if you have 4 GPUs, and 2 nodes, and this is configured as 128
  # Each GPU will process 128/4/2 = 16 datasamples per step, via accumulate_grad_batches
  target_batch_size: 16

########################################
## Training model settings
########################################
model:
  # Model to start the finetune/training process from
  load_model: ../model/L24-D2048-world-v5base-init.pth

  # Context length to use for the training process
  # the larger the number (and batch size) the larger the vram usage
  # 
  # Note that if the datasample context length is larger then the ctx_len
  # its training process would be split into ctx_len sized chunks.
  #
  # This allows the training of extreamly large context length (eg. 100k),
  # without eating up too much vram by keeping the training context length
  # to a resonable number sutible to the current GPU setup
  ctx_len: 2048
  
  # Data samples would be cut down to the respective max ctx_len_cutoffs
  # values if its larger then ctx_len. If the data sample is larger then
  # the largest len_cutoff, the remaining data will be discarded
  ctx_len_cutoffs: []
  # Experimental settings, number of tokens to skip in the data sample
  # prefix, for the respective cutoff length. Used to speed up the process
  ctx_len_warmup_steps: []

  # Learning rate of the training process
  # ---

  # Initia learning rate of the process
  lr_init: 8e-4
  # Final learning rate after the learning rate period
  # learning rate will stay at final value from then onwards
  lr_final: 4e-4

  # Number of epoch to reduce the learning rate from lr_init to lr_final
  #  1 means a single epoch (so lr would be lr_final from epoch 2 onwards)
  #  0 means lr_final will apply immediately
  # -1 means we take the current max_step / max_epoch as the period
  lr_period: 1
  # lr_period type if its set, defaults to epoch
  lr_period_type: epoch

  # Adam optimizer settings
  # You probably want to leave this alone, unless you know what you are doing
  beta1: 0.9
  beta2: 0.99
  adam_eps: 1.0e-08
  weight_decay: 0.01

  # torch.set_float32_matmul_precision, used to optimize operations with tensor cores
  # this should be set as null, for non cuda core GPUs
  torch_set_float32_matmul_precision: 'high'
  # torch_set_float32_matmul_precision: null

  # Segmented based learning, used to work around training of large context length
  # beyond what can be supported by the current GPU vram architecture
  #
  # This is not 1:1 equivalent to the same training process with required vram
  # as the training process is split into multiple segments, part by part.
  # with limited learnings from the previous segment.
  bptt_learning: true

  # Segmented range to performing backprop learning on
  # 1 means to apply only for the last segment
  # -1 means to apply for all segments
  bptt_learning_range: -1

data:
  # datapack_config_path, use a datapack config file directly
  datapack_config_path: ../notebook/trainer-v5-validation/config/datapack-partial-build.yaml

  # Skip any setup of the datapath, assuming the datapath is already setup
  skip_datapath_setup: true

# Path to the current checkpoint to continue training from
# Enable this to the last checkpoint after the first run 
# (if it crash and you want to resume)
# ckpt_path: ../checkpoint/trainer-validaiton/infctx-unit-test-baseline/epoch=0-step=20.ckpt
ckpt_path: null
