{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the datapack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "DEEPSPEED_STRAT: deepspeed_stage_2\n",
      "TRAINING_CTX_LEN: 4096\n",
      "NOTEBOOK_DIR: /workspace/picocreator/RWKV-infctx-trainer/notebook/rwkv-x-exp/v5-exp/expansion-test\n",
      "TRAINER_DIR: /workspace/picocreator/RWKV-infctx-trainer/RWKV-v5\n",
      "PROJECT_DIR: /workspace/picocreator/RWKV-infctx-trainer\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Your configurable settings\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# WANDB settings\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Baseline\"\n",
    "WANDB_PROJECT=\"RWKV-x-v5-Expansion-Test\"\n",
    "\n",
    "# Project directory offset (you need to modify if, you move the notebook into another dir)\n",
    "PROJECT_DIR_OFFSET=\"../../../../\"\n",
    "\n",
    "# GPU count to use\n",
    "GPU_DEVICES=\"auto\"\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# # Training settings you can use to override the \"auto\" default above\n",
    "# -----------------------------------------------------------------\n",
    "DEEPSPEED_STRAT=\"deepspeed_stage_2\"\n",
    "TRAINING_CTX_LEN=4096\n",
    "MICROBATCH_SIZE=8\n",
    "\n",
    "# ---\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"TRAINING_CTX_LEN:\", TRAINING_CTX_LEN)\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, PROJECT_DIR_OFFSET))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(TRAINER_DIR):\n",
    "    raise Exception(\"The trainer directory does not exists. Did you move the notebook?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map (num_proc=160): 100%|███| 1000000/1000000 [00:18<00:00, 54863.81 examples/s]\n",
      "Filter (num_proc=160): 100%|█| 1000000/1000000 [00:06<00:00, 163552.93 examples/\n",
      "Map (num_proc=160): 100%|█████| 648479/648479 [00:10<00:00, 61206.17 examples/s]\n",
      "Map (num_proc=160): 100%|████████| 32835/32835 [00:06<00:00, 5159.39 examples/s]\n",
      "Saving the dataset (7/7 shards): 100%|█| 32835/32835 [00:09<00:00, 3548.78 examp\n",
      "Saving the dataset (1/1 shards): 100%|█| 165/165 [00:00<00:00, 3786.28 examples/\n"
     ]
    }
   ],
   "source": [
    "# Lets build the dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/config/enwiki-100k-world-16k-rechunk.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-11 09:07:45,896] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "---- Initializing model ----\n",
      "No of layers: 32\n",
      "Embedding size: 4096\n",
      "Output model path: ../model/L32-D4096-world-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n",
      "[2024-02-11 09:07:49,619] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "---- Initializing model ----\n",
      "No of layers: 32\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L32-D2048-world-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n",
      "[2024-02-11 09:07:53,474] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "---- Initializing model ----\n",
      "No of layers: 32\n",
      "Embedding size: 1024\n",
      "Output model path: ../model/L32-D1024-world-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n",
      "[2024-02-11 09:07:57,208] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV infctx using 'torch-jit' with torch '2.1.1+cu121'\n",
      "---- Initializing model ----\n",
      "No of layers: 32\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L32-D512-world-init.pth\n",
      "Vocab size: 65536\n",
      "Emb scale: 0.0001\n",
      "Note: this process takes a significant time (and ram) for large models\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Lets initialized the various models, and their sizes\n",
    "!cd \"{TRAINER_DIR}\" && python3 init_model.py \\\n",
    "    --n_layer 32 --n_embd 4096 \\\n",
    "    --vocab_size world \\\n",
    "    --skip-if-exists --safe-init \\\n",
    "    ../model/L32-D4096-world-init.pth\n",
    "\n",
    "!cd \"{TRAINER_DIR}\" && python3 init_model.py \\\n",
    "    --n_layer 32 --n_embd 2048 \\\n",
    "    --vocab_size world \\\n",
    "    --skip-if-exists --safe-init \\\n",
    "    ../model/L32-D2048-world-init.pth\n",
    "\n",
    "!cd \"{TRAINER_DIR}\" && python3 init_model.py \\\n",
    "    --n_layer 32 --n_embd 1024 \\\n",
    "    --vocab_size world \\\n",
    "    --skip-if-exists --safe-init \\\n",
    "    ../model/L32-D1024-world-init.pth\n",
    "\n",
    "!cd \"{TRAINER_DIR}\" && python3 init_model.py \\\n",
    "    --n_layer 32 --n_embd 512 \\\n",
    "    --vocab_size world \\\n",
    "    --skip-if-exists --safe-init \\\n",
    "    ../model/L32-D512-world-init.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
